import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix

import tensorflow as tf
from tensorflow.keras.layers import (Input, Dense, Conv1D, MaxPooling1D, Flatten, Dropout,
                                     LeakyReLU, BatchNormalization, LayerNormalization, MultiHeadAttention)
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.optimizers import Adam

# ============================
# 1. Load and Preprocess Data
# ============================
# Load train/test datasets (adjust file paths as needed)
train = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')

# Extract features and labels (assuming the last two columns are not features)
X = train.iloc[:, :-2]
y = train['Activity'].values  # original labels as strings

# Encode labels into integers
le = LabelEncoder()
y = le.fit_transform(y)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Select top 125 features using ExtraTreesClassifier
feat_selector = ExtraTreesClassifier(n_estimators=100, random_state=42)
feat_selector.fit(X_scaled, y)
importances = feat_selector.feature_importances_
top_indices = np.argsort(importances)[-125:]
X_reduced = X_scaled[:, top_indices]

# ================================
# 2. Synthetic Data Generation via GAN
# ================================
# Define GAN parameters
noise_dim = 100
data_dim = 125  # after feature selection

# Build Generator model
def build_generator(noise_dim, data_dim):
    model = Sequential()
    model.add(Dense(128, input_dim=noise_dim))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(256))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(data_dim, activation='tanh'))  # assume data is scaled
    return model

# Build Discriminator model
def build_discriminator(data_dim):
    model = Sequential()
    model.add(Dense(256, input_dim=data_dim))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dense(128))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dense(1, activation='sigmoid'))
    return model

generator = build_generator(noise_dim, data_dim)
discriminator = build_discriminator(data_dim)
discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])

# Combined GAN model (freeze discriminator during generator training)
z = Input(shape=(noise_dim,))
synthetic_sample = generator(z)
discriminator.trainable = False
validity = discriminator(synthetic_sample)
combined_gan = Model(z, validity)
combined_gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))

# GAN Training parameters
epochs = 500      # adjust epochs as needed
batch_size = 128
real_data = X_reduced

# GAN Training Loop
for epoch in range(epochs):
    # ---------------------
    #  Train Discriminator
    # ---------------------
    idx = np.random.randint(0, real_data.shape[0], batch_size)
    real_samples = real_data[idx]

    noise = np.random.normal(0, 1, (batch_size, noise_dim))
    synthetic_samples = generator.predict(noise)

    d_loss_real = discriminator.train_on_batch(real_samples, np.ones((batch_size, 1)))
    d_loss_fake = discriminator.train_on_batch(synthetic_samples, np.zeros((batch_size, 1)))
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

    # ---------------------
    #  Train Generator
    # ---------------------
    noise = np.random.normal(0, 1, (batch_size, noise_dim))
    g_loss = combined_gan.train_on_batch(noise, np.ones((batch_size, 1)))

    # Print every 100 epochs
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, D loss: {d_loss[0]:.4f}, G loss: {g_loss:.4f}")

# Generate 200 synthetic samples after GAN training
noise = np.random.normal(0, 1, (200, noise_dim))
synthetic_data = generator.predict(noise)

# For demonstration, assign synthetic labels randomly from the real label set.
unique_labels = np.unique(y)
synthetic_labels = np.random.choice(unique_labels, size=200)

# Augment original training data with synthetic data
X_augmented = np.concatenate((X_reduced, synthetic_data), axis=0)
y_augmented = np.concatenate((y, synthetic_labels), axis=0)

# ============================
# 3. Define the Combined Model (CNN + Transformer)
# ============================
def create_combined_model(input_shape, num_classes):
    inputs = Input(shape=input_shape)

    # --- CNN Branch ---
    cnn = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)
    cnn = MaxPooling1D(pool_size=2)(cnn)
    cnn = Dropout(0.3)(cnn)
    cnn = Flatten()(cnn)

    # --- Transformer Branch ---
    transformer = MultiHeadAttention(num_heads=4, key_dim=64)(inputs, inputs)
    transformer = Dropout(0.1)(transformer)
    transformer = LayerNormalization(epsilon=1e-6)(transformer)
    transformer = Flatten()(transformer)

    # --- Merge Branches ---
    combined = tf.keras.layers.concatenate([cnn, transformer])
    dense = Dense(64, activation='relu')(combined)
    outputs = Dense(num_classes, activation='softmax')(dense)

    model = Model(inputs, outputs)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# ============================
# 4. Cross Validation & Evaluation with Confusion Matrices
# ============================
num_classes = len(unique_labels)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Function to reshape data for our model: (samples, sequence_length, channels)
def reshape_for_model(X):
    return X.reshape(-1, data_dim, 1)

X_augmented_reshaped = reshape_for_model(X_augmented)

# Accumulate predictions for final confusion matrix
all_true = []
all_pred = []
fold = 1

for train_index, val_index in skf.split(X_augmented, y_augmented):
    print(f"\nFold {fold}")
    X_train, X_val = X_augmented[train_index], X_augmented[val_index]
    y_train, y_val = y_augmented[train_index], y_augmented[val_index]

    X_train_reshaped = reshape_for_model(X_train)
    X_val_reshaped = reshape_for_model(X_val)

    model = create_combined_model((data_dim, 1), num_classes)
    # Train model and show validation metrics per epoch
    history = model.fit(X_train_reshaped, y_train,
                        epochs=100,
                        batch_size=128,
                        validation_data=(X_val_reshaped, y_val),
                        verbose=1)
    
    # Print the final validation accuracy for the fold
    final_val_acc = history.history['val_accuracy'][-1]
    print(f"Fold {fold} Final Validation Accuracy: {final_val_acc:.4f}")

    # Predict on validation data
    y_val_pred = np.argmax(model.predict(X_val_reshaped), axis=1)
    all_true.extend(y_val)
    all_pred.extend(y_val_pred)

    fold += 1

# Compute the raw confusion matrix
cm = confusion_matrix(all_true, all_pred)
print("\nRaw Confusion Matrix:")
print(cm)

# Compute normalized confusion matrix (normalized by true labels)
cm_norm = confusion_matrix(all_true, all_pred, normalize='true')
print("\nNormalized Confusion Matrix:")
print(cm_norm)

# Plot confusion matrices
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")

plt.subplot(1, 2, 2)
sns.heatmap(cm_norm, annot=True, fmt=".2f", cmap='Blues')
plt.title("Normalized Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")

plt.tight_layout()
plt.show()
